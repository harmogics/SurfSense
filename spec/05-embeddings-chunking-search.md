# Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Chunking, ĞŸĞ¾Ğ¸ÑĞº Ğ¸ Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²

## Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ

Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ñ‚Ñ€ĞµÑ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²:
1. **Embeddings & Vector Search** - Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº
2. **Chunking & Indexing** - Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ
3. **Connectors** - Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…

## Ğ§ĞĞ¡Ğ¢Ğ¬ 1: Embeddings Ğ¸ Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ ĞŸĞ¾Ğ¸ÑĞº

### 1.1 ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Embedding Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹

**ĞœĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ**: `surfsense_backend/app/config/__init__.py:161-183`

```python
from app.embeddings.auto_embeddings import AutoEmbeddings

class Config:
    # Embedding model configuration
    EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")

    # Auto-initialization
    embedding_model_instance = AutoEmbeddings.get_embeddings(
        EMBEDDING_MODEL,
        azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
        azure_api_key=os.getenv("AZURE_OPENAI_API_KEY"),
        openai_api_key=os.getenv("OPENAI_API_KEY")
    )

    # Dimension varies by model
    # text-embedding-3-small: 1536 dimensions
    # text-embedding-3-large: 3072 dimensions
    # text-embedding-ada-002: 1536 dimensions
```

### 1.2 ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ Embedding Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸

| ĞœĞ¾Ğ´ĞµĞ»ÑŒ | ĞŸÑ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€ | Ğ Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ | Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ |
|--------|-----------|-------------|---------------|
| **text-embedding-3-small** | OpenAI | 1536 | ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ/ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ |
| **text-embedding-3-large** | OpenAI | 3072 | ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ |
| **text-embedding-ada-002** | OpenAI | 1536 | Legacy, ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ |
| **Azure OpenAI embeddings** | Azure | 1536/3072 | Ğ”Ğ»Ñ enterprise Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹ |
| **Custom models** | HuggingFace/Local | Variable | Domain-specific embeddings |

### 1.3 Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Embeddings

#### Document-level embedding (summary)

```python
# Ğ’ generate_document_summary()
from app.config import config

# 1. Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ conceptual summary Ñ‡ĞµÑ€ĞµĞ· LLM
summary_content = await llm.ainvoke(SUMMARY_PROMPT_TEMPLATE.format(...))

# 2. ĞĞ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸
enhanced_summary = f"{metadata_markdown}\n{summary_content}"

# 3. Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ embedding
embedding = config.embedding_model_instance.embed(enhanced_summary)
# Output: list[float] Ñ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ 1536 Ğ¸Ğ»Ğ¸ 3072

# 4. Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ² Document
document.embedding = embedding
```

#### Chunk-level embeddings

```python
# Ğ’ create_document_chunks()
chunks = config.chunker_instance.chunk(content)

for chunk in chunks:
    # Embedding Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ chunk
    chunk_embedding = config.embedding_model_instance.embed(chunk.text)

    chunk_obj = Chunk(
        content=chunk.text,
        embedding=chunk_embedding,
        document_id=document.id
    )
    session.add(chunk_obj)
```

### 1.4 Ğ¥Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ² PostgreSQL + pgvector

**Database Schema**:
```python
from pgvector.sqlalchemy import Vector

class Document(Base):
    embedding = Column(Vector(1536), nullable=True)  # Dimension = model dimension

class Chunk(Base):
    embedding = Column(Vector(1536), nullable=True)
```

**Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ğ´ĞµĞºÑĞ° Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°**:
```sql
-- IVFFlat index Ğ´Ğ»Ñ approximate nearest neighbor search
CREATE INDEX ON documents USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);

CREATE INDEX ON chunks USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);
```

### 1.5 Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº

#### Vector Search

**Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ**: `vector_search()`
**ĞœĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ**: `surfsense_backend/app/retriver/chunks_hybrid_search.py:11-61`

```python
async def vector_search(
    session: AsyncSession,
    query_text: str,
    top_k: int,
    user_id: str,
    search_space_id: int
) -> list[Chunk]:
    """
    Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ cosine similarity.

    Process:
    1. Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ query embedding
    2. ĞŸĞ¾Ğ¸ÑĞº Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞ¸Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ² pgvector
    3. Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ user_id Ğ¸ search_space_id
    4. Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚ top_k Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²

    Distance Metric: Cosine distance (<=> operator)
    - distance = 1 - cosine_similarity
    - Lower distance = higher similarity
    """
    from app.config import config

    # 1. Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ embedding Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°
    query_embedding = config.embedding_model_instance.embed(query_text)

    # 2. SQL query Ñ pgvector
    result = await session.execute(
        select(Chunk)
        .join(Document)
        .where(
            Document.user_id == user_id,
            Document.search_space_id == search_space_id
        )
        .order_by(Chunk.embedding.op("<=>")(query_embedding))  # Cosine distance
        .limit(top_k)
    )

    return result.scalars().all()
```

**Cosine distance operator** (<=>):
```
distance = 1 - (A Â· B) / (||A|| * ||B||)

Range: [0, 2]
- 0: identical vectors (perfect match)
- 1: orthogonal (no similarity)
- 2: opposite vectors
```

### 1.6 Full-Text Search

**Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ**: `full_text_search()`
**ĞœĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ**: `surfsense_backend/app/retriver/chunks_hybrid_search.py:63-113`

```python
async def full_text_search(
    session: AsyncSession,
    query_text: str,
    top_k: int,
    user_id: str,
    search_space_id: int
) -> list[Chunk]:
    """
    ĞŸĞ¾Ğ»Ğ½Ğ¾Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ‡ĞµÑ€ĞµĞ· PostgreSQL FTS.

    Process:
    1. ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ°Ñ†Ğ¸Ñ query Ğ² tsquery
    2. ĞŸĞ¾Ğ¸ÑĞº Ğ¿Ğ¾ tsvector Ğ¸Ğ½Ğ´ĞµĞºÑÑƒ
    3. Ğ Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ ts_rank_cd
    4. Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚ top_k Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²

    Advantages:
    - Keyword matching (exact terms)
    - Boolean operators support
    - Language-aware stemming
    """
    from sqlalchemy import func

    # 1. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ tsvector Ğ¸ tsquery
    tsvector = func.to_tsvector('english', Chunk.content)
    tsquery = func.plainto_tsquery('english', query_text)

    # 2. FTS query
    result = await session.execute(
        select(Chunk)
        .join(Document)
        .where(
            Document.user_id == user_id,
            Document.search_space_id == search_space_id,
            tsvector.op('@@')(tsquery)  # Match operator
        )
        .order_by(func.ts_rank_cd(tsvector, tsquery).desc())  # Relevance ranking
        .limit(top_k)
    )

    return result.scalars().all()
```

### 1.7 Hybrid Search Ñ RRF

**Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ**: `hybrid_search()`
**ĞœĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ**: `surfsense_backend/app/retriver/chunks_hybrid_search.py:115-266`

**ĞšĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ**: ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ vector search Ğ¸ full-text search Ñ‡ĞµÑ€ĞµĞ· Reciprocal Rank Fusion.

```python
async def hybrid_search(
    session: AsyncSession,
    query_text: str,
    top_k: int,
    user_id: str,
    search_space_id: int,
    alpha: float = 0.5  # Balance between semantic and keyword
) -> list[Chunk]:
    """
    Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ RRF (Reciprocal Rank Fusion).

    Algorithm:
    1. Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ semantic search (vector) â†’ ranks
    2. Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ keyword search (FTS) â†’ ranks
    3. Compute RRF score for each result
    4. Merge and re-rank by RRF score
    5. Return top_k

    RRF Formula:
    score(chunk) = Î± * (1/(k + rank_semantic)) + (1-Î±) * (1/(k + rank_keyword))
    where k = 60 (constant)

    Benefits:
    - Combines semantic understanding with keyword precision
    - Robust to outliers in individual rankings
    - No parameter tuning required (besides Î±)
    """
    from sqlalchemy import literal, func

    # Constants
    K = 60  # RRF constant

    # Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ query embedding
    query_embedding = config.embedding_model_instance.embed(query_text)

    # CTE #1: Semantic Search
    semantic_cte = (
        select(
            Chunk.id.label('chunk_id'),
            func.row_number().over(
                order_by=Chunk.embedding.op("<=>")(query_embedding)
            ).label('semantic_rank')
        )
        .join(Document)
        .where(
            Document.user_id == user_id,
            Document.search_space_id == search_space_id
        )
        .limit(top_k * 2)  # Get more results for better fusion
        .cte('semantic_results')
    )

    # CTE #2: Keyword Search
    tsvector = func.to_tsvector('english', Chunk.content)
    tsquery = func.plainto_tsquery('english', query_text)

    keyword_cte = (
        select(
            Chunk.id.label('chunk_id'),
            func.row_number().over(
                order_by=func.ts_rank_cd(tsvector, tsquery).desc()
            ).label('keyword_rank')
        )
        .join(Document)
        .where(
            Document.user_id == user_id,
            Document.search_space_id == search_space_id,
            tsvector.op('@@')(tsquery)
        )
        .limit(top_k * 2)
        .cte('keyword_results')
    )

    # FULL OUTER JOIN + RRF scoring
    rrf_query = (
        select(
            func.coalesce(semantic_cte.c.chunk_id, keyword_cte.c.chunk_id).label('chunk_id'),
            (
                alpha * (1.0 / (K + func.coalesce(semantic_cte.c.semantic_rank, 1000))) +
                (1 - alpha) * (1.0 / (K + func.coalesce(keyword_cte.c.keyword_rank, 1000)))
            ).label('rrf_score')
        )
        .select_from(
            semantic_cte.outerjoin(
                keyword_cte,
                semantic_cte.c.chunk_id == keyword_cte.c.chunk_id,
                full=True
            )
        )
        .order_by(literal_column('rrf_score').desc())
        .limit(top_k)
    )

    # Execute and fetch chunks
    result = await session.execute(rrf_query)
    chunk_ids = [row.chunk_id for row in result]

    # Fetch full chunk objects
    chunks = await session.execute(
        select(Chunk).where(Chunk.id.in_(chunk_ids))
    )

    # Preserve RRF ordering
    chunks_dict = {chunk.id: chunk for chunk in chunks.scalars()}
    ordered_chunks = [chunks_dict[cid] for cid in chunk_ids if cid in chunks_dict]

    return ordered_chunks
```

**ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ RRF Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ**:
```
Query: "database performance optimization"

Semantic Search Results:        Keyword Search Results:
1. chunk_42 (rank 1)             1. chunk_108 (rank 1)
2. chunk_108 (rank 2)            2. chunk_42 (rank 2)
3. chunk_205 (rank 3)            3. chunk_315 (rank 3)

RRF Scores (k=60, Î±=0.5):
chunk_42:  0.5*(1/61) + 0.5*(1/62) = 0.0164
chunk_108: 0.5*(1/62) + 0.5*(1/61) = 0.0164
chunk_205: 0.5*(1/63) + 0.5*(1/1060) = 0.0079
chunk_315: 0.5*(1/1060) + 0.5*(1/63) = 0.0079

Merged Ranking:
1. chunk_42 (tie)
2. chunk_108 (tie)
3. chunk_205
4. chunk_315
```

---

## Ğ§ĞĞ¡Ğ¢Ğ¬ 2: Chunking Ğ¸ Ğ˜Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ

### 2.1 Chunking ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ

**ĞœĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ**: `surfsense_backend/app/config/__init__.py:178-183`

```python
from chonkie import RecursiveChunker, CodeChunker

# Text chunker
chunker_instance = RecursiveChunker(
    chunk_size=getattr(embedding_model_instance, "max_seq_length", 512)
)

# Code chunker (Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°)
code_chunker_instance = CodeChunker(
    chunk_size=getattr(embedding_model_instance, "max_seq_length", 512)
)
```

**Adaptive Chunk Size**:
- Ğ Ğ°Ğ·Ğ¼ĞµÑ€ chunk Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¿Ğ¾Ğ´ embedding Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
- text-embedding-3-small: max_seq_length â‰ˆ 512 tokens
- text-embedding-3-large: max_seq_length â‰ˆ 512 tokens
- Ğ”Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ

### 2.2 ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Chunking

**Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ**: `create_document_chunks()`
**ĞœĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ**: `surfsense_backend/app/utils/document_converters.py:148-164`

```python
async def create_document_chunks(
    content: str,
    document_id: int | None = None
) -> list[Chunk]:
    """
    Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµÑ‚ chunks Ğ¸Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ñ embeddings.

    Strategy:
    1. Recursive chunking Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹
    2. Overlap Ğ¼ĞµĞ¶Ğ´Ñƒ chunks Ğ´Ğ»Ñ context continuity
    3. Adaptive sizing Ğ¿Ğ¾ embedding model capacity
    4. Embedding generation Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ chunk

    ĞŸÑ€Ğ¾Ñ†ĞµÑÑ:
    1. Chunking ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°
    2. Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ embedding Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ chunk
    3. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Chunk Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²
    4. Ğ¡Ğ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ parent Document
    """
    from app.config import config

    # 1. Chunking
    chunks = config.chunker_instance.chunk(content)

    # 2. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Chunk Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ embeddings
    chunk_objects = []

    for chunk in chunks:
        # Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ embedding
        chunk_embedding = config.embedding_model_instance.embed(chunk.text)

        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°
        chunk_obj = Chunk(
            content=chunk.text,
            embedding=chunk_embedding,
            document_id=document_id
        )

        chunk_objects.append(chunk_obj)

    return chunk_objects
```

### 2.3 RecursiveChunker (ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ)

**ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼**:
```python
def recursive_chunk(text: str, chunk_size: int) -> list[str]:
    """
    Ğ ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹.

    ĞŸÑ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ (Ğ¾Ñ‚ Ğ²Ñ‹ÑÑˆĞµĞ³Ğ¾ Ğº Ğ½Ğ¸Ğ·ÑˆĞµĞ¼Ñƒ):
    1. "\n\n" (Ğ¿Ğ°Ñ€Ğ°Ğ³Ñ€Ğ°Ñ„Ñ‹)
    2. "\n" (ÑÑ‚Ñ€Ğ¾ĞºĞ¸)
    3. ". " (Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ)
    4. ", " (Ğ·Ğ°Ğ¿ÑÑ‚Ñ‹Ğµ)
    5. " " (ÑĞ»Ğ¾Ğ²Ğ°)

    ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°:
    - Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ¼Ñ‹ÑĞ»Ğ¾Ğ²Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
    - ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ñ‹ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹
    - ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°
    """
    separators = ["\n\n", "\n", ". ", ", ", " "]

    for separator in separators:
        if separator in text:
            parts = text.split(separator)
            chunks = []
            current_chunk = ""

            for part in parts:
                if len(current_chunk) + len(part) < chunk_size:
                    current_chunk += part + separator
                else:
                    if current_chunk:
                        chunks.append(current_chunk)
                    current_chunk = part + separator

            if current_chunk:
                chunks.append(current_chunk)

            return chunks

    # Fallback: character-level splitting
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
```

### 2.4 CodeChunker (Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°)

**Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ°**:
```python
class CodeChunker:
    """
    Chunker Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°.

    Features:
    - Function/class boundary awareness
    - Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹
    - Comment preservation
    - Import/dependency tracking
    """

    def chunk(self, code: str, language: str = "python") -> list[str]:
        # ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ AST Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹/ĞºĞ»Ğ°ÑÑĞ¾Ğ²
        # Ğ Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ğ°Ğ¼ (functions, classes, modules)
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ imports Ğ¸ docstrings
        ...
```

### 2.5 Ğ˜Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ Ğ² Ğ‘Ğ”

**Process**:
```python
# ĞŸĞ¾ÑĞ»Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ¸ chunking
async with session.begin():
    # 1. Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Document
    document = Document(
        title=title,
        content=content,
        embedding=summary_embedding,
        document_type=DocumentType.FILE,
        user_id=user_id,
        search_space_id=search_space_id
    )
    session.add(document)
    await session.flush()  # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ document.id

    # 2. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ chunks
    chunks = await create_document_chunks(content, document.id)
    for chunk in chunks:
        session.add(chunk)

    await session.commit()
```

---

## Ğ§ĞĞ¡Ğ¢Ğ¬ 3: Connector Integration (Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²)

### 3.1 ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ Connectors

**Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ**: `surfsense_backend/app/tasks/connector_indexers/`

| Connector | Ğ¢Ğ¸Ğ¿ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… | ĞœĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ |
|-----------|------------|------------|
| **Slack** | Ğ¡Ğ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ, Ñ‚Ñ€ĞµĞ´Ñ‹ | channel, author, timestamp, reactions |
| **GitHub** | Issues, PRs, ĞºĞ¾Ğ´ | repo, author, labels, comments |
| **Notion** | Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹, Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… | workspace, author, last_edited |
| **Jira** | Tasks, issues | project, assignee, status, priority |
| **Confluence** | Wiki ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ | space, author, version |
| **Discord** | Ğ¡Ğ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ, ĞºĞ°Ğ½Ğ°Ğ»Ñ‹ | server, channel, author |
| **Google Calendar** | Ğ¡Ğ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ | organizer, attendees, time |
| **Gmail** | Emails | sender, recipients, subject, date |
| **Linear** | Issues, projects | team, assignee, status |
| **ClickUp** | Tasks, docs | space, assignee, due_date |
| **Airtable** | Records, tables | base, table, fields |

### 3.2 Base Indexer Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸

**Ğ¤Ğ°Ğ¹Ğ»**: `surfsense_backend/app/tasks/connector_indexers/base.py`

```python
async def check_duplicate_document_by_hash(
    session: AsyncSession,
    content_hash: str,
    user_id: str,
    search_space_id: int
) -> Document | None:
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ñ‹ Ğ¿Ğ¾ content_hash"""
    ...

async def check_document_by_unique_identifier(
    session: AsyncSession,
    unique_identifier_hash: str,
    user_id: str,
    search_space_id: int
) -> Document | None:
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ¿Ğ¾ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¾Ğ¼Ñƒ ID"""
    ...

async def update_connector_last_indexed(
    session: AsyncSession,
    connector: Connector
) -> None:
    """ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚ĞºÑƒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ¹ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸"""
    ...
```

### 3.3 ĞŸÑ€Ğ¸Ğ¼ĞµÑ€: Slack Indexer

**Ğ¤Ğ°Ğ¹Ğ»**: `surfsense_backend/app/tasks/connector_indexers/slack_indexer.py`

```python
async def index_slack_messages(
    session: AsyncSession,
    connector_id: int,
    search_space_id: int,
    user_id: str,
    start_date: str | None = None,
    end_date: str | None = None
) -> tuple[int, str | None]:
    """
    Ğ˜Ğ½Ğ´ĞµĞºÑĞ¸Ñ€ÑƒĞµÑ‚ Slack ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ.

    Process:
    1. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Slack connector config
    2. Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Slack client
    3. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¸ÑĞºĞ° ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ²
    4. Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ½Ğ°Ğ»Ğ°:
       - Fetch messages Ğ² Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğµ Ğ´Ğ°Ñ‚
       - ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ² (Ğ¿Ğ¾ message_id)
       - Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Markdown
       - Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ summary + embedding
       - Chunking + chunk embeddings
       - Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ² Ğ‘Ğ”
    5. ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ last_indexed_at

    Metadata:
    {
        "channel": "engineering",
        "channel_id": "C123456",
        "author": "user@example.com",
        "author_id": "U789012",
        "timestamp": "2024-03-15T10:30:00Z",
        "message_url": "https://workspace.slack.com/archives/C123456/p1234567890",
        "has_reactions": true,
        "reactions": ["ğŸ‘": 5, "ğŸ‰": 2],
        "thread_ts": null  # or timestamp if part of thread
    }
    """
    from slack_sdk import WebClient
    import hashlib

    # 1. Get connector
    connector = await get_connector_by_id(session, connector_id, ConnectorType.SLACK)
    slack_token = connector.credentials['access_token']
    slack_client = WebClient(token=slack_token)

    # 2. Calculate date range
    start_ts, end_ts = calculate_date_range(connector, start_date, end_date)

    # 3. Get channels
    channels = slack_client.conversations_list()['channels']

    indexed_count = 0

    for channel in channels:
        channel_id = channel['id']
        channel_name = channel['name']

        # 4. Fetch messages
        messages = slack_client.conversations_history(
            channel=channel_id,
            oldest=start_ts,
            latest=end_ts
        )['messages']

        for message in messages:
            # Unique identifier Ğ´Ğ»Ñ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸
            unique_id = f"slack:{channel_id}:{message['ts']}"
            unique_id_hash = hashlib.sha256(unique_id.encode()).hexdigest()

            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ²
            existing = await check_document_by_unique_identifier(
                session, unique_id_hash, user_id, search_space_id
            )
            if existing:
                continue  # Ğ£Ğ¶Ğµ Ğ¸Ğ½Ğ´ĞµĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾

            # Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ message
            content = format_slack_message_to_markdown(message, channel_name)

            # Metadata
            metadata = {
                "channel": channel_name,
                "channel_id": channel_id,
                "author": message.get('user'),
                "timestamp": message['ts'],
                "message_url": f"https://{connector.workspace}.slack.com/archives/{channel_id}/p{message['ts'].replace('.', '')}",
                "reactions": extract_reactions(message)
            }

            # Content hash
            content_hash = hashlib.sha256(content.encode()).hexdigest()

            # Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ summary + embedding
            summary, embedding = await generate_document_summary(
                content=content,
                user_llm=get_user_llm(session, user_id),
                document_metadata=metadata
            )

            # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Document
            document = Document(
                title=f"Slack: {channel_name} - {message.get('user')}",
                content=content,
                document_type=DocumentType.SLACK,
                document_metadata=metadata,
                content_hash=content_hash,
                unique_identifier_hash=unique_id_hash,
                embedding=embedding,
                user_id=user_id,
                search_space_id=search_space_id,
                connector_id=connector_id
            )
            session.add(document)
            await session.flush()

            # Chunking
            chunks = await create_document_chunks(content, document.id)
            for chunk in chunks:
                session.add(chunk)

            indexed_count += 1

    # 5. Update last_indexed
    await update_connector_last_indexed(session, connector)
    await session.commit()

    return indexed_count, None
```

### 3.4 Connector Service (Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼)

**Ğ¤Ğ°Ğ¹Ğ»**: `surfsense_backend/app/services/connector_service.py`

```python
class ConnectorService:
    async def search_all_sources(
        self,
        user_query: str,
        user_id: str,
        search_space_id: int,
        top_k: int = 20,
        search_mode: SearchMode = SearchMode.CHUNKS
    ) -> SearchResults:
        """
        ĞŸĞ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼ (files, urls, connectors).

        Returns:
            SearchResults with documents/chunks and metadata
        """
        # Hybrid search
        results = await hybrid_search(
            session=self.session,
            query_text=user_query,
            top_k=top_k,
            user_id=user_id,
            search_space_id=search_space_id
        )

        # Group by source type
        grouped_results = self.group_by_source_type(results)

        return SearchResults(
            documents=results,
            sources=grouped_results,
            total_count=len(results)
        )
```

---

## Ğ ĞµĞ·ÑĞ¼Ğµ: ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               COMPLETE INDEXING & SEARCH PIPELINE           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SOURCE INGESTION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Files / URLs / Connectors â†’ ETL Processing â†’ Markdown

           â†“

CONCEPTUAL EXTRACTION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  LLM Summarization â†’ Conceptual Summary + Metadata

           â†“

EMBEDDING GENERATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Document Embedding (summary)
  + Chunk Embeddings (content)

           â†“

CHUNKING
â”€â”€â”€â”€â”€â”€â”€â”€
  RecursiveChunker / CodeChunker â†’ Semantic chunks

           â†“

DATABASE STORAGE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  PostgreSQL + pgvector
  - Document table (with embedding)
  - Chunk table (with embeddings)
  - Indexes for fast retrieval

           â†“

SEARCH & RETRIEVAL
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Query â†’ Embedding â†’ Hybrid Search (Vector + FTS + RRF)
                   â†“
              Reranking (Cohere, Pinecone)
                   â†“
              Top-K Results

           â†“

AI AGENT PROCESSING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Results â†’ Q&A Agent â†’ Answer with Citations
```

## ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ñ‹

| ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ | Ğ¤Ğ°Ğ¹Ğ» | ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ |
|-----------|------|------------------|
| **Embeddings** | `config/__init__.py` | `embedding_model_instance` |
| **Vector Search** | `retriver/chunks_hybrid_search.py` | `vector_search()`, `hybrid_search()` |
| **Chunking** | `utils/document_converters.py` | `create_document_chunks()` |
| **Indexers** | `tasks/connector_indexers/*` | Connector-specific indexing |
| **Search Service** | `services/connector_service.py` | `search_all_sources()` |

---

## ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸

### Ğ˜Ğ½Ğ´ĞµĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
- **Batch processing**: Ğ˜Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±Ğ°Ñ‚Ñ‡Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ overhead
- **Async operations**: Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ async/await Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸
- **Caching**: ĞšĞµÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ embeddings Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ¸Ñ…ÑÑ chunks

### ĞŸĞ¾Ğ¸ÑĞº
- **pgvector IVFFlat**: Approximate nearest neighbor Ğ´Ğ»Ñ O(log N) Ğ²Ğ¼ĞµÑÑ‚Ğ¾ O(N)
- **Hybrid RRF**: ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ precision/recall
- **Reranking**: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° 15-30%
