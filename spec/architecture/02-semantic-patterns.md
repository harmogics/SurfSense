# Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹

## Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ

Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ² SurfSense Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ². Ğ­Ñ‚Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ embeddings, Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ AI Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ñ‹ÑĞ»Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ².

## 1. RAG Pattern (Retrieval-Augmented Generation)

### ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ

RAG (Retrieval-Augmented Generation) - ÑÑ‚Ğ¾ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (Retrieval) Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚Ğ° (Generation), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ LLM.

### ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° RAG

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  RAG PIPELINE                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   User Query
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. RETRIEVAL   â”‚  â† Hybrid Search (Vector + FTS + RRF)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Retrieved Documents
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. AUGMENTATION â”‚  â† Context preparation + Token optimization
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Augmented Context
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. GENERATION  â”‚  â† LLM with context â†’ Answer
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Generated Answer
         â–¼
    Response with
     Citations
```

### Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² SurfSense

**Ğ¤Ğ°Ğ¹Ğ»**: `surfsense_backend/app/agents/researcher/nodes.py`

#### Phase 1: Retrieval

```python
async def handle_qna_workflow(
    state: State,
    config: RunnableConfig,
    writer: StreamWriter
) -> dict:
    """
    RAG Pattern implementation Ğ² Q&A workflow.
    """
    configuration = Configuration.from_runnable_config(config)
    streaming_service = state.streaming_service

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 1: RETRIEVAL
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # 1.1. ĞŸĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ³Ğ¾ ÑƒĞ·Ğ»Ğ°
    user_query = state.reformulated_query

    # 1.2. Fetch relevant documents using hybrid search
    writer({
        "yield_value": streaming_service.format_terminal_info_delta(
            "ğŸ” Searching for relevant documents..."
        )
    })

    # Create connector service Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ°
    connector_service = ConnectorService(
        state.db_session,
        user_id=configuration.user_id
    )
    await connector_service.initialize_counter()

    # Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼
    relevant_documents = await fetch_relevant_documents(
        research_questions=[user_query],
        user_id=configuration.user_id,
        search_space_id=configuration.search_space_id,
        db_session=state.db_session,
        connectors_to_search=configuration.connectors_to_search,
        writer=writer,
        state=state,
        top_k=20,  # Retrieve top 20 initially
        connector_service=connector_service,
        search_mode=SearchMode.CHUNKS
    )

    writer({
        "yield_value": streaming_service.format_terminal_info_delta(
            f"âœ… Found {len(relevant_documents)} relevant documents",
            message_type="success"
        )
    })

    # 1.3. Combine with user-selected documents (if any)
    user_selected_documents = configuration.user_selected_documents or []
    all_documents = user_selected_documents + relevant_documents

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 2: AUGMENTATION & GENERATION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # 2.1. Pass to Q&A SubAgent for augmentation and generation
    qna_agent_graph = build_qna_graph()

    qna_state = {
        "user_query": user_query,
        "relevant_documents": all_documents,  # AUGMENTATION: provide context
        "user_id": configuration.user_id,
        "search_space_id": configuration.search_space_id,
        "db_session": state.db_session,
        "streaming_service": streaming_service
    }

    qna_config = {
        "configurable": {
            "user_query": user_query,
            "relevant_documents": all_documents,
            "user_id": configuration.user_id,
            "search_space_id": configuration.search_space_id
        }
    }

    # 2.2. Execute Q&A SubAgent (handles augmentation and generation)
    complete_content = ""
    sources = []

    async for chunk_type, chunk in qna_agent_graph.astream(qna_state, qna_config):
        if chunk_type == "rerank_documents":
            # Reranked documents ready
            writer({
                "yield_value": streaming_service.format_terminal_info_delta(
                    "ğŸ“Š Reranked documents by relevance"
                )
            })

        elif chunk_type == "answer_question":
            if "delta" in chunk:
                # Stream generated answer chunks
                writer({
                    "yield_value": streaming_service.format_text_chunk(
                        chunk["delta"]
                    )
                })
                complete_content += chunk["delta"]

            if "final_answer" in chunk:
                complete_content = chunk["final_answer"]

            if "sources" in chunk:
                sources = chunk["sources"]

    # Stream sources
    if sources:
        writer({
            "yield_value": streaming_service.format_sources_delta(sources)
        })

    return {"final_written_report": complete_content}
```

#### Phase 2: Augmentation (Ğ² Q&A SubAgent)

**Ğ¤Ğ°Ğ¹Ğ»**: `surfsense_backend/app/agents/researcher/qna_agent/nodes.py`

```python
async def answer_question(
    state: QnAState,
    config: RunnableConfig,
    writer: StreamWriter
) -> dict:
    """
    RAG: Augmentation + Generation.
    """
    configuration = Configuration.from_runnable_config(config)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 2: AUGMENTATION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # 2.1. Get reranked documents from previous node
    reranked_documents = state.reranked_documents  # Top 10 after reranking

    # 2.2. Optimize documents for token limit
    fast_llm = await get_user_fast_llm(
        state.db_session,
        configuration.user_id,
        configuration.search_space_id
    )

    # Get model's context window
    model_info = litellm.get_model_info(fast_llm.model)
    max_context_tokens = model_info.get("max_input_tokens", 8192)

    # Reserve tokens for prompt and output
    PROMPT_TOKENS = 500
    OUTPUT_TOKENS = 2000
    available_tokens = max_context_tokens - PROMPT_TOKENS - OUTPUT_TOKENS

    # 2.3. Format documents with token optimization
    formatted_context = ""
    sources_metadata = []
    current_tokens = 0

    for idx, doc in enumerate(reranked_documents, 1):
        doc_text = f"\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
        doc_text += f"[{idx}] {doc['document']['title']}\n"
        doc_text += f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
        doc_text += doc["content"]

        # Count tokens
        doc_tokens = litellm.token_counter(
            model=fast_llm.model,
            text=doc_text
        )

        if current_tokens + doc_tokens > available_tokens:
            # Truncate document to fit
            remaining_tokens = available_tokens - current_tokens
            if remaining_tokens > 100:  # Minimum viable chunk
                truncated = truncate_to_tokens(doc_text, remaining_tokens)
                formatted_context += truncated
            break

        formatted_context += doc_text
        current_tokens += doc_tokens

        # Track source for citations
        sources_metadata.append({
            "citation_number": idx,
            "document_id": doc["document"]["id"],
            "chunk_id": doc["chunk_id"],
            "title": doc["document"]["title"],
            "url": doc["document"].get("url"),
            "type": doc["document"]["document_type"]
        })

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 3: GENERATION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # 3.1. Build QA prompt with augmented context
    from app.prompts import QA_PROMPT_TEMPLATE

    qa_prompt = QA_PROMPT_TEMPLATE.format(
        user_query=state.user_query,
        context=formatted_context  # AUGMENTED CONTEXT
    )

    # 3.2. Generate answer with LLM
    complete_answer = ""

    async for chunk in fast_llm.astream(qa_prompt, temperature=0.3):
        delta = chunk.content

        # Stream to user
        writer({
            "yield_value": state.streaming_service.format_text_chunk(delta)
        })

        complete_answer += delta

    # 3.3. Extract citations from answer
    cited_sources = extract_citations_from_answer(
        complete_answer,
        sources_metadata
    )

    return {
        "final_answer": complete_answer,
        "sources": cited_sources
    }
```

### QA Prompt Template

**Ğ¤Ğ°Ğ¹Ğ»**: `surfsense_backend/app/prompts/__init__.py`

```python
QA_PROMPT_TEMPLATE = """
You are a knowledgeable research assistant. Answer the user's question based on the provided context documents.

## Context Documents
{context}

## User's Question
{user_query}

## Instructions
1. **Answer Accuracy**: Base your answer strictly on the provided context
2. **Citations**: Cite your sources using [1], [2], [3] format after relevant statements
3. **Comprehensiveness**: Provide a detailed and complete answer
4. **Structure**: Use markdown formatting (headers, lists, code blocks) for clarity
5. **Objectivity**: Present information objectively without personal opinions
6. **Limitations**: If the context doesn't contain sufficient information, acknowledge it

## Output Format
Provide a well-structured answer with:
- Clear introduction
- Main content with appropriate citations [N]
- Conclusion or summary if applicable
- Code examples if relevant (with proper syntax highlighting)

Answer:
"""
```

### ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° RAG Pattern

1. **ĞĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ**: Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ±Ğ°Ğ·Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹
2. **Ğ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ**: ĞÑ‚Ğ²ĞµÑ‚Ñ‹ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ñ„Ğ°ĞºÑ‚Ğ°Ñ… Ğ¸Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
3. **ĞŸÑ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ**: Ğ¦Ğ¸Ñ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸
4. **ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ**: LLM Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°
5. **ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ**: ĞĞµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…

---

## 2. Embedding Pipeline Pattern

### ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ

Embedding Pipeline Pattern Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ (embeddings) Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°.

### ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              EMBEDDING PIPELINE                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   Raw Text
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Preprocessingâ”‚  â† Normalization, cleaning
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Normalized Text
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tokenization â”‚  â† Split into tokens
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Tokens
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embedding    â”‚  â† text-embedding-3-small/large
â”‚ Model        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Vector[1536/3072]
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Storage      â”‚  â† PostgreSQL + pgvector
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² SurfSense

**Ğ¤Ğ°Ğ¹Ğ»**: `surfsense_backend/app/retriver/chunks_hybrid_search.py`

#### Vector Search Ñ Embedding Pipeline

```python
from app.config import config

async def vector_search(
    self,
    query_text: str,
    top_k: int,
    user_id: str,
    search_space_id: int
) -> list:
    """
    Embedding Pipeline Ğ´Ğ»Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°.

    Pipeline:
    1. Query text â†’ Embedding model â†’ Query vector
    2. Query vector â†’ PostgreSQL pgvector â†’ Similar vectors
    3. Similar vectors â†’ Documents/Chunks
    """

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 1: EMBEDDING GENERATION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # Get embedding model instance from global config
    embedding_model = config.embedding_model_instance

    # Convert query text to vector
    query_embedding = embedding_model.embed(query_text)
    # Output: list[float] with length 1536 or 3072

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 2: VECTOR SIMILARITY SEARCH
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # Build SQL query with pgvector operators
    from sqlalchemy import select
    from app.db import Chunk, Document, SearchSpace

    query = (
        select(Chunk)
        .join(Document, Chunk.document_id == Document.id)
        .join(SearchSpace, Document.search_space_id == SearchSpace.id)
        .where(
            SearchSpace.user_id == user_id,
            SearchSpace.id == search_space_id
        )
        # pgvector: cosine distance operator (<=>)
        .order_by(Chunk.embedding.op("<=>")(query_embedding))
        .limit(top_k)
    )

    # Execute query
    result = await self.db_session.execute(query)
    chunks = result.scalars().all()

    return chunks
```

#### Embedding Model Configuration

**Ğ¤Ğ°Ğ¹Ğ»**: `surfsense_backend/app/config/__init__.py`

```python
from app.embeddings.auto_embeddings import AutoEmbeddings
import os

class Config:
    """Global configuration including embedding model"""

    # Embedding model selection
    EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")

    # Azure OpenAI configuration (if using Azure)
    AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
    AZURE_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")

    # OpenAI configuration
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

    # Initialize embedding model instance
    embedding_model_instance = AutoEmbeddings.get_embeddings(
        EMBEDDING_MODEL,
        azure_endpoint=AZURE_OPENAI_ENDPOINT,
        azure_api_key=AZURE_API_KEY,
        openai_api_key=OPENAI_API_KEY
    )

    # Get embedding dimension from model
    EMBEDDING_DIMENSION = getattr(
        embedding_model_instance,
        "dimension",
        1536  # Default for text-embedding-3-small
    )
```

#### Document Embedding (Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸)

**Ğ¤Ğ°Ğ¹Ğ»**: `surfsense_backend/app/utils/document_converters.py`

```python
async def generate_document_summary(
    content: str,
    user_llm: Any,
    document_metadata: dict | None = None
) -> tuple[str, list[float]]:
    """
    Generate summary and embedding for document.

    Embedding Pipeline:
    1. Content â†’ LLM â†’ Summary
    2. Summary + Metadata â†’ Enhanced summary
    3. Enhanced summary â†’ Embedding model â†’ Vector
    """
    from app.config import config

    # Step 1: Generate summary via LLM
    optimized_content = optimize_content_for_context_window(
        content,
        document_metadata,
        user_llm.model_name
    )

    from app.prompts import SUMMARY_PROMPT_TEMPLATE
    summary_prompt = SUMMARY_PROMPT_TEMPLATE.format(
        metadata=format_metadata(document_metadata),
        content=optimized_content
    )

    summary_response = await user_llm.ainvoke(summary_prompt)
    summary_content = summary_response.content

    # Step 2: Enhance summary with metadata
    metadata_markdown = format_metadata_as_markdown(document_metadata)
    enhanced_summary = f"{metadata_markdown}\n\n{summary_content}"

    # Step 3: Generate embedding
    # EMBEDDING PIPELINE: Text â†’ Vector
    embedding_model = config.embedding_model_instance
    summary_embedding = embedding_model.embed(enhanced_summary)

    return summary_content, summary_embedding
```

#### Chunk Embedding

**Ğ¤Ğ°Ğ¹Ğ»**: `surfsense_backend/app/utils/document_converters.py`

```python
async def create_document_chunks(
    content: str,
    document_id: int | None = None
) -> list:
    """
    Create chunks with embeddings.

    Embedding Pipeline for each chunk:
    1. Content â†’ Chunker â†’ Chunks
    2. Each chunk â†’ Embedding model â†’ Chunk vector
    """
    from app.config import config

    # Step 1: Chunking
    chunker = config.chunker_instance
    chunks = chunker.chunk(content)

    # Step 2: Embed each chunk
    chunk_objects = []

    for chunk in chunks:
        # EMBEDDING PIPELINE: Chunk text â†’ Vector
        chunk_embedding = config.embedding_model_instance.embed(chunk.text)

        chunk_obj = Chunk(
            content=chunk.text,
            embedding=chunk_embedding,  # Vector[1536/3072]
            document_id=document_id
        )

        chunk_objects.append(chunk_obj)

    return chunk_objects
```

### ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ Embedding Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸

| ĞœĞ¾Ğ´ĞµĞ»ÑŒ | Ğ Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ | ĞŸÑ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€ | Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ |
|--------|-------------|-----------|---------------|
| **text-embedding-3-small** | 1536 | OpenAI | Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ/ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ |
| **text-embedding-3-large** | 3072 | OpenAI | ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ |
| **text-embedding-ada-002** | 1536 | OpenAI | Legacy, ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ°Ñ |
| **Azure embeddings** | 1536/3072 | Azure OpenAI | Enterprise Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ |

### ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Embedding Pipeline

1. **Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ**: Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¼Ñ‹ÑĞ», Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ»Ğ¾Ğ²Ğ°
2. **ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ**: Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸
3. **ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ**: ĞĞ´Ğ¸Ğ½ Ñ€Ğ°Ğ· Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼, Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼
4. **Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ**: pgvector IVFFlat Ğ¸Ğ½Ğ´ĞµĞºÑÑ‹ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°
5. **Ğ“Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ**: Ğ›ĞµĞ³ĞºĞ¾ ÑĞ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ embeddings

---

## 3. Hybrid Search Pattern

### ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ

Hybrid Search Pattern ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²:
- **Vector Search**: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ
- **Full-Text Search**: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ²
- **RRF (Reciprocal Rank Fusion)**: Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²

### ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Hybrid Search

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              HYBRID SEARCH PATTERN                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        Query Text
            â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â–¼                  â–¼                  â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Optional:
    â”‚ Vector Searchâ”‚   â”‚ Full-Text    â”‚   Filters
    â”‚ (Semantic)   â”‚   â”‚ Search (FTS) â”‚   (metadata,
    â”‚              â”‚   â”‚              â”‚    date, etc.)
    â”‚ embed(query) â”‚   â”‚ to_tsquery   â”‚
    â”‚ cosine <=>   â”‚   â”‚ ts_rank_cd   â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                  â”‚
           â”‚ Results (ranked) â”‚ Results (ranked)
           â”‚                  â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Reciprocal Rank â”‚
            â”‚ Fusion (RRF)    â”‚
            â”‚                 â”‚
            â”‚ score = 1/(k+râ‚)â”‚
            â”‚       + 1/(k+râ‚‚)â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
            Merged & Ranked
             Results (top_k)
```

### Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² SurfSense

**Ğ¤Ğ°Ğ¹Ğ»**: `surfsense_backend/app/retriver/chunks_hybrid_search.py`

#### Hybrid Search Ñ RRF

```python
async def hybrid_search(
    self,
    query_text: str,
    top_k: int,
    user_id: str,
    search_space_id: int,
    document_type: str | None = None
) -> list:
    """
    Hybrid Search: Vector + Full-Text + RRF.

    Algorithm:
    1. Perform semantic search (vector) â†’ ranked results
    2. Perform keyword search (FTS) â†’ ranked results
    3. Apply RRF (Reciprocal Rank Fusion) â†’ merged ranking
    4. Return top_k results
    """
    from sqlalchemy import select, func, literal, text
    from app.config import config

    # Configuration
    K = 60  # RRF constant
    n_results = top_k * 2  # Get more for better fusion

    # Get query embedding for semantic search
    embedding_model = config.embedding_model_instance
    query_embedding = embedding_model.embed(query_text)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CTE 1: SEMANTIC SEARCH (Vector Similarity)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    semantic_search_cte = (
        select(
            Chunk.id.label('chunk_id'),
            # Assign rank based on similarity
            func.row_number()
            .over(order_by=Chunk.embedding.op("<=>")(query_embedding))
            .label('semantic_rank')
        )
        .join(Document, Chunk.document_id == Document.id)
        .join(SearchSpace, Document.search_space_id == SearchSpace.id)
        .where(
            SearchSpace.user_id == user_id,
            SearchSpace.id == search_space_id
        )
        .order_by(Chunk.embedding.op("<=>")(query_embedding))
        .limit(n_results)
        .cte('semantic_search')
    )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CTE 2: KEYWORD SEARCH (Full-Text Search)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # Build tsvector and tsquery
    tsvector = func.to_tsvector('english', Chunk.content)
    tsquery = func.plainto_tsquery('english', query_text)

    keyword_search_cte = (
        select(
            Chunk.id.label('chunk_id'),
            # Assign rank based on FTS relevance
            func.row_number()
            .over(order_by=func.ts_rank_cd(tsvector, tsquery).desc())
            .label('keyword_rank')
        )
        .join(Document, Chunk.document_id == Document.id)
        .join(SearchSpace, Document.search_space_id == SearchSpace.id)
        .where(
            SearchSpace.user_id == user_id,
            SearchSpace.id == search_space_id,
            tsvector.op('@@')(tsquery)  # Full-text match operator
        )
        .order_by(func.ts_rank_cd(tsvector, tsquery).desc())
        .limit(n_results)
        .cte('keyword_search')
    )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # RRF: RECIPROCAL RANK FUSION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # FULL OUTER JOIN + RRF score calculation
    rrf_query = (
        select(
            # Get chunk_id from either CTE
            func.coalesce(
                semantic_search_cte.c.chunk_id,
                keyword_search_cte.c.chunk_id
            ).label('chunk_id'),

            # RRF Score Formula:
            # score = 1/(k + rank_semantic) + 1/(k + rank_keyword)
            # If a result appears in only one method, the missing rank = 1000 (low score)
            (
                1.0 / (K + func.coalesce(semantic_search_cte.c.semantic_rank, 1000)) +
                1.0 / (K + func.coalesce(keyword_search_cte.c.keyword_rank, 1000))
            ).label('rrf_score')
        )
        .select_from(
            semantic_search_cte.outerjoin(
                keyword_search_cte,
                semantic_search_cte.c.chunk_id == keyword_search_cte.c.chunk_id,
                full=True  # FULL OUTER JOIN
            )
        )
        .order_by(text('rrf_score DESC'))
        .limit(top_k)
    )

    # Execute RRF query
    result = await self.db_session.execute(rrf_query)
    rrf_results = result.fetchall()

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # FETCH FULL CHUNK OBJECTS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    chunk_ids = [row.chunk_id for row in rrf_results]
    rrf_scores = {row.chunk_id: row.rrf_score for row in rrf_results}

    # Fetch full chunk data
    chunks_query = (
        select(Chunk, Document)
        .join(Document, Chunk.document_id == Document.id)
        .where(Chunk.id.in_(chunk_ids))
    )

    chunks_result = await self.db_session.execute(chunks_query)
    chunks = chunks_result.all()

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # FORMAT RESULTS WITH SCORES
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    serialized_results = []

    for chunk, document in chunks:
        serialized_results.append({
            "chunk_id": chunk.id,
            "content": chunk.content,
            "score": rrf_scores.get(chunk.id, 0.0),  # RRF score
            "document": {
                "id": document.id,
                "title": document.title,
                "document_type": document.document_type.value,
                "url": document.document_metadata.get("url"),
                # ... other metadata
            }
        })

    # Sort by RRF score (descending) and preserve order
    serialized_results.sort(key=lambda x: x["score"], reverse=True)

    return serialized_results[:top_k]
```

### RRF (Reciprocal Rank Fusion) Formula

```
For each document d:

score(d) = 1/(k + rank_semantic(d)) + 1/(k + rank_keyword(d))

where:
- k = 60 (constant, reduces impact of high ranks)
- rank_semantic(d) = rank in vector search results (1-indexed)
- rank_keyword(d) = rank in full-text search results (1-indexed)
- If d not in one of the methods: rank = 1000 (very low contribution)
```

**ĞŸÑ€Ğ¸Ğ¼ĞµÑ€**:
```
Query: "database performance optimization"

Semantic Search Results:       Keyword Search Results:
1. chunk_42 (rank 1)            1. chunk_108 (rank 1)
2. chunk_108 (rank 2)           2. chunk_42 (rank 2)
3. chunk_205 (rank 3)           3. chunk_315 (rank 3)
4. chunk_99 (rank 4)            [chunk_205 not in results]

RRF Scores (k=60):
chunk_42:  1/(60+1) + 1/(60+2) = 0.0164 + 0.0161 = 0.0325
chunk_108: 1/(60+2) + 1/(60+1) = 0.0161 + 0.0164 = 0.0325
chunk_205: 1/(60+3) + 1/(60+1000) = 0.0159 + 0.0009 = 0.0168
chunk_315: 1/(60+1000) + 1/(60+3) = 0.0009 + 0.0159 = 0.0168
chunk_99:  1/(60+4) + 1/(60+1000) = 0.0156 + 0.0009 = 0.0165

Final Ranking:
1. chunk_42 (0.0325) - appears high in both
2. chunk_108 (0.0325) - appears high in both
3. chunk_205 (0.0168) - strong in semantic only
4. chunk_315 (0.0168) - strong in keyword only
5. chunk_99 (0.0165) - moderate in semantic
```

### ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Hybrid Search

1. **Best of both worlds**: Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ° + Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ
2. **Robustness**: ĞšĞ¾Ğ¼Ğ¿ĞµĞ½ÑĞ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°
3. **No parameter tuning**: RRF Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ²ĞµÑĞ¾Ğ²
4. **Domain agnostic**: Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
5. **Proven effectiveness**: RRF Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ…

---

## 4. Reranking Pattern

### ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ

Reranking Pattern ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€Ğ²Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ².

### ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              RERANKING PATTERN                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Query + Initial Search Results (top 20)
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ First-stage   â”‚  â† Hybrid Search (fast)
        â”‚ Retrieval     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ Top 20 candidates
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Reranking     â”‚  â† Cross-encoder model (slow but accurate)
        â”‚ Model         â”‚     - Cohere rerank
        â”‚               â”‚     - Pinecone rerank
        â”‚               â”‚     - Custom models
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ Reranked top 10
                â–¼
        Better ranked results
```

### Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² SurfSense

**Ğ¤Ğ°Ğ¹Ğ»**: `surfsense_backend/app/services/reranker_service.py`

#### RerankerService

```python
from rerankers import Reranker
from typing import Optional

class RerankerService:
    """
    Service for reranking search results using cross-encoder models.
    """

    def __init__(self, reranker_instance=None):
        self.reranker_instance = reranker_instance

    def rerank_documents(
        self,
        query_text: str,
        documents: list[dict]
    ) -> list[dict]:
        """
        Rerank documents using cross-encoder model.

        Process:
        1. Convert documents to reranker format
        2. Call reranker model with query and documents
        3. Update scores with reranker scores
        4. Return reranked results

        Args:
            query_text: User query
            documents: List of documents with initial scores (from hybrid search)

        Returns:
            Reranked documents with updated scores
        """
        if not self.reranker_instance or not documents:
            return documents  # Fallback to original

        try:
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # STEP 1: CONVERT TO RERANKER FORMAT
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

            reranker_docs = []

            for doc in documents:
                from rerankers.documents import RerankerDocument

                reranker_docs.append(
                    RerankerDocument(
                        text=doc.get("content", ""),
                        doc_id=doc.get("chunk_id"),
                        metadata={
                            "document_id": doc.get("document", {}).get("id"),
                            "original_score": doc.get("score", 0.0),  # RRF score
                            "title": doc.get("document", {}).get("title")
                        }
                    )
                )

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # STEP 2: CALL RERANKER MODEL
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

            reranking_results = self.reranker_instance.rank(
                query=query_text,
                docs=reranker_docs
            )

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # STEP 3: UPDATE SCORES
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

            serialized_results = []

            for result in reranking_results.results:
                # Find original document
                original_doc = next(
                    (d for d in documents if d["chunk_id"] == result.document.doc_id),
                    None
                )

                if original_doc:
                    # Create new document with reranker score
                    reranked_doc = original_doc.copy()

                    # UPDATE: Replace hybrid search score with reranker score
                    reranked_doc["score"] = float(result.score)
                    reranked_doc["rank"] = result.rank
                    reranked_doc["original_score"] = original_doc.get("score", 0.0)

                    serialized_results.append(reranked_doc)

            return serialized_results

        except Exception as e:
            logger.error(f"Reranking failed: {str(e)}")
            return documents  # Fallback to original on error

    @staticmethod
    def get_reranker_instance() -> Optional["RerankerService"]:
        """
        Get reranker service instance from global config.

        Reranker models:
        - Cohere: rerank-english-v3.0, rerank-multilingual-v3.0
        - Pinecone: bge-reranker-v2-m3
        - Custom: any cross-encoder model via HuggingFace
        """
        from app.config import config

        if hasattr(config, "reranker_instance") and config.reranker_instance:
            return RerankerService(config.reranker_instance)

        return None
```

#### Configuration

**Ğ¤Ğ°Ğ¹Ğ»**: `surfsense_backend/app/config/__init__.py`

```python
class Config:
    # Reranker configuration
    RERANKERS_ENABLED = os.getenv("RERANKERS_ENABLED", "false").lower() == "true"
    RERANKERS_MODEL_NAME = os.getenv("RERANKERS_MODEL_NAME", "")
    RERANKERS_MODEL_TYPE = os.getenv("RERANKERS_MODEL_TYPE", "")  # cohere, pinecone, etc.

    # Initialize reranker instance if enabled
    reranker_instance = None
    if RERANKERS_ENABLED and RERANKERS_MODEL_NAME:
        from rerankers import Reranker

        if RERANKERS_MODEL_TYPE == "cohere":
            reranker_instance = Reranker(
                RERANKERS_MODEL_NAME,
                api_key=os.getenv("COHERE_API_KEY")
            )
        elif RERANKERS_MODEL_TYPE == "pinecone":
            reranker_instance = Reranker(RERANKERS_MODEL_NAME)
        else:
            # Default: load from HuggingFace
            reranker_instance = Reranker(RERANKERS_MODEL_NAME)
```

#### Usage in Q&A Agent

**Ğ¤Ğ°Ğ¹Ğ»**: `surfsense_backend/app/agents/researcher/qna_agent/nodes.py`

```python
async def rerank_documents(
    state: QnAState,
    config: RunnableConfig,
    writer: StreamWriter
) -> dict:
    """
    Rerank documents using RerankerService.

    Input: state.relevant_documents (top 20 from hybrid search)
    Output: state.reranked_documents (top 10 after reranking)
    """
    from app.services.reranker_service import RerankerService

    # Check if reranking is enabled
    reranker_service = RerankerService.get_reranker_instance()

    if not reranker_service:
        # Reranking disabled, return top 10 from hybrid search
        return {"reranked_documents": state.relevant_documents[:10]}

    # Rerank top 20 results
    reranked_results = reranker_service.rerank_documents(
        query_text=state.user_query,
        documents=state.relevant_documents  # Top 20 from hybrid search
    )

    # Take top 10 after reranking
    top_reranked = reranked_results[:10]

    return {"reranked_documents": top_reranked}
```

### Reranking Models

| ĞœĞ¾Ğ´ĞµĞ»ÑŒ | Ğ¢Ğ¸Ğ¿ | Ğ¯Ğ·Ñ‹ĞºĞ¸ | ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ |
|--------|-----|-------|----------|
| **Cohere rerank-english-v3.0** | Cross-encoder | English | ĞÑ‡ĞµĞ½ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ |
| **Cohere rerank-multilingual-v3.0** | Cross-encoder | 100+ languages | Ğ’Ñ‹ÑĞ¾ĞºĞ¾Ğµ |
| **Pinecone bge-reranker-v2-m3** | Cross-encoder | Multilingual | Ğ’Ñ‹ÑĞ¾ĞºĞ¾Ğµ |
| **Custom HF models** | Cross-encoder | Varies | Varies |

### ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Reranking Pattern

1. **Improved precision**: +15-30% improvement over hybrid search alone
2. **Better relevance**: Cross-encoders understand query-document relationship
3. **Trade-off**: Slower but more accurate (applied to small candidate set)
4. **Plug-and-play**: Easy to enable/disable via configuration
5. **Fallback safety**: Automatically falls back to original scores on error

---

## Ğ ĞµĞ·ÑĞ¼Ğµ: Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹

| ĞŸĞ°Ñ‚Ñ‚ĞµÑ€Ğ½ | Ğ¤Ğ°Ğ¹Ğ» | ĞĞ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ | ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ |
|---------|------|------------|---------------------|
| **RAG** | `agents/researcher/nodes.py` | Context-aware generation | Retrieval â†’ Augmentation â†’ Generation |
| **Embedding Pipeline** | `retriver/chunks_hybrid_search.py` | Text vectorization | Text â†’ Model â†’ Vector â†’ Storage |
| **Hybrid Search** | `retriver/chunks_hybrid_search.py` | Best-of-both search | Vector + FTS + RRF |
| **Reranking** | `services/reranker_service.py` | Improve precision | Initial search â†’ Cross-encoder â†’ Top results |

Ğ­Ñ‚Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹, Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ.
